{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u_cXsyFtuPB7"
   },
   "source": [
    "# Large-scale multi-label text classification\n",
    "\n",
    "**Author:** [Sayak Paul](https://twitter.com/RisingSayak), [Soumik Rakshit](https://github.com/soumik12345)<br>\n",
    "**Date created:** 2020/09/25<br>\n",
    "**Last modified:** 2020/09/26<br>\n",
    "**Description:** Implementing a large-scale multi-label text classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9jwS32LBuPCB"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this example, we will build a multi-label text classifier to predict the subject areas\n",
    "of arXiv papers from their abstract bodies. This type of classifier can be useful for\n",
    "conference submission portals like [OpenReview](https://openreview.net/). Given a paper\n",
    "abstract, the portal could provide suggestions for which areas the paper would\n",
    "best belong to.\n",
    "\n",
    "The dataset was collected using the\n",
    "[`arXiv` Python library](https://github.com/lukasschwab/arxiv.py)\n",
    "that provides a wrapper around the\n",
    "[original arXiv API](http://arxiv.org/help/api/index).\n",
    "To learn more about the data collection process, please refer to\n",
    "[this notebook](https://github.com/soumik12345/multi-label-text-classification/blob/master/arxiv_scrape.ipynb).\n",
    "Additionally, you can also find the dataset on\n",
    "[Kaggle](https://www.kaggle.com/spsayakpaul/arxiv-paper-abstracts)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RoZ4uRYNuPCB"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "PI-RJ-2xuPCC"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-04 18:06:26.784244: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-12-04 18:06:26.784266: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ast import literal_eval\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V6hzCuyruPCE"
   },
   "source": [
    "## Perform exploratory data analysis\n",
    "\n",
    "In this section, we first load the dataset into a `pandas` dataframe and then perform\n",
    "some basic exploratory data analysis (EDA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "PTki4xc1uPCE"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>titles</th>\n",
       "      <th>summaries</th>\n",
       "      <th>terms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Survey on Semantic Stereo Matching / Semantic ...</td>\n",
       "      <td>Stereo matching is one of the widely used tech...</td>\n",
       "      <td>['cs.CV', 'cs.LG']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FUTURE-AI: Guiding Principles and Consensus Re...</td>\n",
       "      <td>The recent advancements in artificial intellig...</td>\n",
       "      <td>['cs.CV', 'cs.AI', 'cs.LG']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Enforcing Mutual Consistency of Hard Regions f...</td>\n",
       "      <td>In this paper, we proposed a novel mutual cons...</td>\n",
       "      <td>['cs.CV', 'cs.AI']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Parameter Decoupling Strategy for Semi-supervi...</td>\n",
       "      <td>Consistency training has proven to be an advan...</td>\n",
       "      <td>['cs.CV']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Background-Foreground Segmentation for Interio...</td>\n",
       "      <td>To ensure safety in automated driving, the cor...</td>\n",
       "      <td>['cs.CV', 'cs.LG']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              titles  \\\n",
       "0  Survey on Semantic Stereo Matching / Semantic ...   \n",
       "1  FUTURE-AI: Guiding Principles and Consensus Re...   \n",
       "2  Enforcing Mutual Consistency of Hard Regions f...   \n",
       "3  Parameter Decoupling Strategy for Semi-supervi...   \n",
       "4  Background-Foreground Segmentation for Interio...   \n",
       "\n",
       "                                           summaries  \\\n",
       "0  Stereo matching is one of the widely used tech...   \n",
       "1  The recent advancements in artificial intellig...   \n",
       "2  In this paper, we proposed a novel mutual cons...   \n",
       "3  Consistency training has proven to be an advan...   \n",
       "4  To ensure safety in automated driving, the cor...   \n",
       "\n",
       "                         terms  \n",
       "0           ['cs.CV', 'cs.LG']  \n",
       "1  ['cs.CV', 'cs.AI', 'cs.LG']  \n",
       "2           ['cs.CV', 'cs.AI']  \n",
       "3                    ['cs.CV']  \n",
       "4           ['cs.CV', 'cs.LG']  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_data = pd.read_csv(\n",
    "    \"https://github.com/soumik12345/multi-label-text-classification/releases/download/v0.2/arxiv_data.csv\"\n",
    ")\n",
    "arxiv_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T6QDE1wkuPCF"
   },
   "source": [
    "Our text features are present in the `summaries` column and their corresponding labels\n",
    "are in `terms`. As you can notice, there are multiple categories associated with a\n",
    "particular entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Rq-T6GkXuPCG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 51774 rows in the dataset.\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {len(arxiv_data)} rows in the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QZ_E3gMWuPCG"
   },
   "source": [
    "Real-world data is noisy. One of the most commonly observed source of noise is data\n",
    "duplication. Here we notice that our initial dataset has got about 13k duplicate entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Dx9JFz4BuPCH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 12802 duplicate titles.\n"
     ]
    }
   ],
   "source": [
    "total_duplicate_titles = sum(arxiv_data[\"titles\"].duplicated())\n",
    "print(f\"There are {total_duplicate_titles} duplicate titles.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UzJjBGHtuPCI"
   },
   "source": [
    "Before proceeding further, we drop these entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "nUIAGSZHuPCJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 38972 rows in the deduplicated dataset.\n",
      "2321\n",
      "3157\n"
     ]
    }
   ],
   "source": [
    "arxiv_data = arxiv_data[~arxiv_data[\"titles\"].duplicated()]\n",
    "print(f\"There are {len(arxiv_data)} rows in the deduplicated dataset.\")\n",
    "\n",
    "# There are some terms with occurrence as low as 1.\n",
    "print(sum(arxiv_data[\"terms\"].value_counts() == 1))\n",
    "\n",
    "# How many unique terms?\n",
    "print(arxiv_data[\"terms\"].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BKKBusjnuPCK"
   },
   "source": [
    "As observed above, out of 3,157 unique combinations of `terms`, 2,321 entries have the\n",
    "lowest occurrence. To prepare our train, validation, and test sets with\n",
    "[stratification](https://en.wikipedia.org/wiki/Stratified_sampling), we need to drop\n",
    "these terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "-n-HtHtuuPCK"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36651, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filtering the rare terms.\n",
    "arxiv_data_filtered = arxiv_data.groupby(\"terms\").filter(lambda x: len(x) > 1)\n",
    "arxiv_data_filtered.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1bQT9lvsuPCL"
   },
   "source": [
    "## Convert the string labels to lists of strings\n",
    "\n",
    "The initial labels are represented as raw strings. Here we make them `List[str]` for a\n",
    "more compact representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "poeDsbxNuPCL"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['cs.CV', 'cs.LG']), list(['cs.CV', 'cs.AI', 'cs.LG']),\n",
       "       list(['cs.CV', 'cs.AI']), list(['cs.CV']),\n",
       "       list(['cs.CV', 'cs.LG'])], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_data_filtered[\"terms\"] = arxiv_data_filtered[\"terms\"].apply(\n",
    "    lambda x: literal_eval(x)\n",
    ")\n",
    "arxiv_data_filtered[\"terms\"].values[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9rMd13FiuPCL"
   },
   "source": [
    "## Use stratified splits because of class imbalance\n",
    "\n",
    "The dataset has a\n",
    "[class imbalance problem](https://developers.google.com/machine-learning/glossary/#class-imbalanced-dataset).\n",
    "So, to have a fair evaluation result, we need to ensure the datasets are sampled with\n",
    "stratification. To know more about different strategies to deal with the class imbalance\n",
    "problem, you can follow\n",
    "[this tutorial](https://www.tensorflow.org/tutorials/structured_data/imbalanced_data).\n",
    "For an end-to-end demonstration of classification with imbablanced data, refer to\n",
    "[Imbalanced classification: credit card fraud detection](https://keras.io/examples/structured_data/imbalanced_classification/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "-7ckHS7PuPCM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in training set: 32985\n",
      "Number of rows in validation set: 1833\n",
      "Number of rows in test set: 1833\n"
     ]
    }
   ],
   "source": [
    "test_split = 0.1\n",
    "\n",
    "# Initial train and test split.\n",
    "train_df, test_df = train_test_split(\n",
    "    arxiv_data_filtered,\n",
    "    test_size=test_split,\n",
    "    stratify=arxiv_data_filtered[\"terms\"].values,\n",
    ")\n",
    "\n",
    "# Splitting the test set further into validation\n",
    "# and new test sets.\n",
    "val_df = test_df.sample(frac=0.5)\n",
    "test_df.drop(val_df.index, inplace=True)\n",
    "\n",
    "print(f\"Number of rows in training set: {len(train_df)}\")\n",
    "print(f\"Number of rows in validation set: {len(val_df)}\")\n",
    "print(f\"Number of rows in test set: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TkY9W2M4uPCM"
   },
   "source": [
    "## Multi-label binarization\n",
    "\n",
    "Now we preprocess our labels using the\n",
    "[`StringLookup`](https://keras.io/api/layers/preprocessing_layers/categorical/string_lookup)\n",
    "layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "A2YgvZ56uPCN"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-04 18:07:19.788021: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-12-04 18:07:19.788053: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-12-04 18:07:19.788073: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (arch-desktop): /proc/driver/nvidia/version does not exist\n",
      "2021-12-04 18:07:19.788629: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-12-04 18:07:19.885044: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:\n",
      "\n",
      "['[UNK]', 'cs.CV', 'cs.LG', 'stat.ML', 'cs.AI', 'eess.IV', 'cs.RO', 'cs.CL', 'cs.NE', 'cs.CR', 'math.OC', 'eess.SP', 'cs.GR', 'cs.SI', 'cs.MM', 'cs.SY', 'cs.IR', 'cs.MA', 'eess.SY', 'cs.HC', 'math.IT', 'cs.IT', 'cs.DC', 'cs.CY', 'stat.AP', 'stat.TH', 'math.ST', 'stat.ME', 'eess.AS', 'cs.SD', 'q-bio.QM', 'q-bio.NC', 'cs.DS', 'cs.GT', 'cs.SE', 'cs.NI', 'cs.CG', 'stat.CO', 'I.2.6', 'math.NA', 'cs.NA', 'physics.chem-ph', 'cs.DB', 'q-bio.BM', 'cs.LO', 'cond-mat.dis-nn', '68T45', 'math.PR', 'cs.PL', 'physics.comp-ph', 'cs.CE', 'cs.AR', 'I.2.10', 'q-fin.ST', 'cond-mat.stat-mech', 'math.DS', 'cs.CC', '68T05', 'quant-ph', 'I.4.6', 'physics.data-an', 'physics.soc-ph', 'physics.ao-ph', 'econ.EM', 'cs.DM', 'q-bio.GN', 'physics.med-ph', 'cs.PF', 'astro-ph.IM', 'I.4.8', 'math.AT', 'cs.FL', 'I.4', 'q-fin.TR', 'I.5.4', 'I.2', '68U10', 'hep-ex', 'cond-mat.mtrl-sci', '68T10', 'physics.geo-ph', 'physics.optics', 'physics.flu-dyn', 'math.CO', 'math.AP', 'I.4; I.5', 'I.4.9', 'I.2.6; I.2.8', '68T01', '65D19', 'q-fin.CP', 'nlin.CD', 'cs.MS', 'I.2.6; I.5.1', 'I.2.10; I.4; I.5', 'I.2.0; I.2.6', '68T07', 'cs.SC', 'cs.ET', 'K.3.2', 'I.2.8', '68U01', '68T30', 'q-fin.GN', 'q-fin.EC', 'q-bio.MN', 'econ.GN', 'I.4.9; I.5.4', 'I.4.5', 'I.2; I.5', 'I.2; I.4; I.5', 'I.2.6; I.2.7', 'I.2.10; I.4.8', '68T99', '68Q32', '68', '62H30', 'q-fin.RM', 'q-fin.PM', 'q-bio.TO', 'q-bio.OT', 'physics.bio-ph', 'nlin.AO', 'math.LO', 'math.FA', 'hep-ph', 'cond-mat.soft', 'I.4.6; I.4.8', 'I.4.4', 'I.4.3', 'I.4.0', 'I.2; J.2', 'I.2; I.2.6; I.2.7', 'I.2.7', 'I.2.6; I.5.4', 'I.2.6; I.2.9', 'I.2.6; I.2.7; H.3.1; H.3.3', 'I.2.6; I.2.10', 'I.2.6, I.5.4', 'I.2.1; J.3', 'I.2.10; I.5.1; I.4.8', 'I.2.10; I.4.8; I.5.4', 'I.2.10; I.2.6', 'I.2.1', 'H.3.1; I.2.6; I.2.7', 'H.3.1; H.3.3; I.2.6; I.2.7', 'G.3', 'F.2.2; I.2.7', 'E.5; E.4; E.2; H.1.1; F.1.1; F.1.3', '68Txx', '62H99', '62H35', '14J60 (Primary) 14F05, 14J26 (Secondary)']\n"
     ]
    }
   ],
   "source": [
    "terms = tf.ragged.constant(train_df[\"terms\"].values)\n",
    "lookup = tf.keras.layers.StringLookup(output_mode=\"multi_hot\")\n",
    "lookup.adapt(terms)\n",
    "vocab = lookup.get_vocabulary()\n",
    "\n",
    "\n",
    "def invert_multi_hot(encoded_labels):\n",
    "    \"\"\"Reverse a single multi-hot encoded label to a tuple of vocab terms.\"\"\"\n",
    "    hot_indices = np.argwhere(encoded_labels == 1.0)[..., 0]\n",
    "    return np.take(vocab, hot_indices)\n",
    "\n",
    "\n",
    "print(\"Vocabulary:\\n\")\n",
    "print(vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wVhu06PQuPCN"
   },
   "source": [
    "Here we are separating the individual unique classes available from the label\n",
    "pool and then using this information to represent a given label set with 0's and 1's.\n",
    "Below is an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vUHPGyir8Tvo"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "IBxYh_05uPCO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original label: ['cs.CV']\n",
      "Label-binarized representation: [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "sample_label = train_df[\"terms\"].iloc[0]\n",
    "print(f\"Original label: {sample_label}\")\n",
    "\n",
    "label_binarized = lookup([sample_label])\n",
    "print(f\"Label-binarized representation: {label_binarized}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "GBJdmeSAudEt"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>titles</th>\n",
       "      <th>summaries</th>\n",
       "      <th>terms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7643</th>\n",
       "      <td>Unsupervised Hard Example Mining from Videos f...</td>\n",
       "      <td>Important gains have recently been obtained in...</td>\n",
       "      <td>[cs.CV]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40357</th>\n",
       "      <td>iPerceive: Applying Common-Sense Reasoning to ...</td>\n",
       "      <td>Most prior art in visual understanding relies ...</td>\n",
       "      <td>[cs.CV, cs.AI, cs.LG, cs.MM, eess.IV]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2914</th>\n",
       "      <td>BBN: Bilateral-Branch Network with Cumulative ...</td>\n",
       "      <td>Our work focuses on tackling the challenging b...</td>\n",
       "      <td>[cs.CV, cs.LG]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25236</th>\n",
       "      <td>Assigning Apples to Individual Trees in Dense ...</td>\n",
       "      <td>We propose a 3D color point cloud processing p...</td>\n",
       "      <td>[cs.CV]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10882</th>\n",
       "      <td>Towards Adversarially Robust and Domain Genera...</td>\n",
       "      <td>Stereo matching has recently witnessed remarka...</td>\n",
       "      <td>[cs.CV, cs.AI]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44051</th>\n",
       "      <td>Matrix Factorization Equals Efficient Co-occur...</td>\n",
       "      <td>Matrix factorization is a simple and effective...</td>\n",
       "      <td>[cs.LG, stat.ML]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22962</th>\n",
       "      <td>Temporal Graph Modeling for Skeleton-based Act...</td>\n",
       "      <td>Graph Convolutional Networks (GCNs), which mod...</td>\n",
       "      <td>[cs.CV, cs.GR]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29731</th>\n",
       "      <td>Deep Hashing for Secure Multimodal Biometrics</td>\n",
       "      <td>When compared to unimodal systems, multimodal ...</td>\n",
       "      <td>[cs.CV, cs.AI, cs.IT, math.IT]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4415</th>\n",
       "      <td>Learning Disentangled Representations via Mutu...</td>\n",
       "      <td>In this paper, we investigate the problem of l...</td>\n",
       "      <td>[stat.ML, cs.LG]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31911</th>\n",
       "      <td>Is Pessimism Provably Efficient for Offline RL?</td>\n",
       "      <td>We study offline reinforcement learning (RL), ...</td>\n",
       "      <td>[cs.LG, cs.AI, math.OC, math.ST, stat.ML, stat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32985 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  titles  \\\n",
       "7643   Unsupervised Hard Example Mining from Videos f...   \n",
       "40357  iPerceive: Applying Common-Sense Reasoning to ...   \n",
       "2914   BBN: Bilateral-Branch Network with Cumulative ...   \n",
       "25236  Assigning Apples to Individual Trees in Dense ...   \n",
       "10882  Towards Adversarially Robust and Domain Genera...   \n",
       "...                                                  ...   \n",
       "44051  Matrix Factorization Equals Efficient Co-occur...   \n",
       "22962  Temporal Graph Modeling for Skeleton-based Act...   \n",
       "29731      Deep Hashing for Secure Multimodal Biometrics   \n",
       "4415   Learning Disentangled Representations via Mutu...   \n",
       "31911    Is Pessimism Provably Efficient for Offline RL?   \n",
       "\n",
       "                                               summaries  \\\n",
       "7643   Important gains have recently been obtained in...   \n",
       "40357  Most prior art in visual understanding relies ...   \n",
       "2914   Our work focuses on tackling the challenging b...   \n",
       "25236  We propose a 3D color point cloud processing p...   \n",
       "10882  Stereo matching has recently witnessed remarka...   \n",
       "...                                                  ...   \n",
       "44051  Matrix factorization is a simple and effective...   \n",
       "22962  Graph Convolutional Networks (GCNs), which mod...   \n",
       "29731  When compared to unimodal systems, multimodal ...   \n",
       "4415   In this paper, we investigate the problem of l...   \n",
       "31911  We study offline reinforcement learning (RL), ...   \n",
       "\n",
       "                                                   terms  \n",
       "7643                                             [cs.CV]  \n",
       "40357              [cs.CV, cs.AI, cs.LG, cs.MM, eess.IV]  \n",
       "2914                                      [cs.CV, cs.LG]  \n",
       "25236                                            [cs.CV]  \n",
       "10882                                     [cs.CV, cs.AI]  \n",
       "...                                                  ...  \n",
       "44051                                   [cs.LG, stat.ML]  \n",
       "22962                                     [cs.CV, cs.GR]  \n",
       "29731                     [cs.CV, cs.AI, cs.IT, math.IT]  \n",
       "4415                                    [stat.ML, cs.LG]  \n",
       "31911  [cs.LG, cs.AI, math.OC, math.ST, stat.ML, stat...  \n",
       "\n",
       "[32985 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "30lIzcFOuPCO"
   },
   "source": [
    "## Data preprocessing and `tf.data.Dataset` objects\n",
    "\n",
    "We first get percentile estimates of the sequence lengths. The purpose will be clear in a\n",
    "moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "5F9b5wSduPCO"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    32985.000000\n",
       "mean       156.499227\n",
       "std         41.489081\n",
       "min          5.000000\n",
       "25%        128.000000\n",
       "50%        154.000000\n",
       "75%        183.000000\n",
       "max        462.000000\n",
       "Name: summaries, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"summaries\"].apply(lambda x: len(x.split(\" \"))).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Km527i1quPCP"
   },
   "source": [
    "Notice that 50% of the abstracts have a length of 154 (you may get a different number\n",
    "based on the split). So, any number close to that value is a good enough approximate for the\n",
    "maximum sequence length.\n",
    "\n",
    "Now, we implement utilities to prepare our datasets that would go straight to the text\n",
    "classifier model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "UKCU8Qi-uPCR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 239869 unique tokens.\n",
      "(32985, 250)\n",
      "(1833, 250)\n"
     ]
    }
   ],
   "source": [
    "#text_vectorizer = layers.TextVectorization(\n",
    "#    max_tokens=vocabulary_size, ngrams=2, output_mode=\"tf_idf\"\n",
    "#)\n",
    "\n",
    "maxWords = 50000\n",
    "maxSeqLen = 250\n",
    "embeddingDim = 150\n",
    "\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=maxWords, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer.fit_on_texts(train_df['summaries'].values)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(train_df['summaries'].values)\n",
    "X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, maxlen=maxSeqLen)\n",
    "\n",
    "X_test= tokenizer.texts_to_sequences(test_df['summaries'].values)\n",
    "X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, maxlen=maxSeqLen)\n",
    "\n",
    "X_val = tokenizer.texts_to_sequences(val_df['summaries'].values)\n",
    "X_val = tf.keras.preprocessing.sequence.pad_sequences(X_val, maxlen=maxSeqLen)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "lDPMjUi4uPCP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 153)\n",
      "(1833, 153)\n",
      "(1833, 153)\n"
     ]
    }
   ],
   "source": [
    "train_labels = tf.ragged.constant(train_df[\"terms\"].values)\n",
    "train_label_binarized = lookup(train_labels).numpy()\n",
    "print(label_binarized.shape)\n",
    "\n",
    "test_labels = tf.ragged.constant(test_df[\"terms\"].values)\n",
    "test_label_binarized = lookup(test_labels).numpy()\n",
    "print(test_label_binarized.shape)\n",
    "\n",
    "val_labels = tf.ragged.constant(val_df[\"terms\"].values)\n",
    "val_label_binarized = lookup(val_labels).numpy()\n",
    "print(val_label_binarized.shape)\n",
    "#    dataset = tf.data.Dataset.from_tensor_slices(\n",
    "#        (dataframe[\"summaries\"].values, label_binarized)\n",
    "#    )\n",
    "#    dataset = dataset.shuffle(batch_size * 10) if is_train else dataset\n",
    "#    dataset = dataset.map(unify_text_length, num_parallel_calls=auto).cache()\n",
    "#    return dataset.batch(batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k-uXT0JVuPCQ"
   },
   "source": [
    "Now we can prepare the `tf.data.Dataset` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "uSdl_gmtuPCQ"
   },
   "outputs": [],
   "source": [
    "#train_dataset = make_dataset(train_df, is_train=True)\n",
    "#validation_dataset = make_dataset(val_df, is_train=False)\n",
    "#test_dataset = make_dataset(test_df, is_train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UG_cbSnsuPCQ"
   },
   "source": [
    "## Dataset preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "WBRKgLuBuPCQ"
   },
   "outputs": [],
   "source": [
    "#text_batch, label_batch = next(iter(train_dataset))\n",
    "\n",
    "#for i, text in enumerate(text_batch[:5]):\n",
    "#    label = label_batch[i].numpy()[None, ...]\n",
    "#    print(f\"Abstract: {text[0]}\")\n",
    "#    print(f\"Label(s): {invert_multi_hot(label[0])}\")\n",
    "#    print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sn2icscHuPCR"
   },
   "source": [
    "## Vectorization\n",
    "\n",
    "Before we feed the data to our model, we need to vectorize it (represent it in a numerical form).\n",
    "For that purpose, we will use the\n",
    "[`TextVectorization` layer](https://keras.io/api/layers/preprocessing_layers/text/text_vectorization).\n",
    "It can operate as a part of your main model so that the model is excluded from the core\n",
    "preprocessing logic. This greatly reduces the chances of training / serving skew during inference.\n",
    "\n",
    "We first calculate the number of unique words present in the abstracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "8a50f3mkuPCR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 498\n"
     ]
    }
   ],
   "source": [
    "train_df[\"total_words\"] = train_df[\"summaries\"].str.split().str.len()\n",
    "vocabulary_size = train_df[\"total_words\"].max()\n",
    "print(f\"Vocabulary size: {vocabulary_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TqrMK3EeuPCR"
   },
   "source": [
    "We now create our vectorization layer and `map()` to the `tf.data.Dataset`s created\n",
    "earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "56SroinguPCS"
   },
   "source": [
    "A batch of raw text will first go through the `TextVectorization` layer and it will\n",
    "generate their integer representations. Internally, the `TextVectorization` layer will\n",
    "first create bi-grams out of the sequences and then represent them using\n",
    "[TF-IDF](https://wikipedia.org/wiki/Tf%E2%80%93idf). The output representations will then\n",
    "be passed to the shallow model responsible for text classification.\n",
    "\n",
    "To learn more about other possible configurations with `TextVectorizer`, please consult\n",
    "the\n",
    "[official documentation](https://keras.io/api/layers/preprocessing_layers/text/text_vectorization).\n",
    "\n",
    "**Note**: Setting the `max_tokens` argument to a pre-calculated vocabulary size is\n",
    "not a requirement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lN3sk6CiuPCS"
   },
   "source": [
    "## Create a text classification model\n",
    "\n",
    "We will keep our model simple -- it will be a small stack of fully-connected layers with\n",
    "ReLU as the non-linearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "G5ITudV3uPCS"
   },
   "outputs": [],
   "source": [
    "\n",
    "def make_model():\n",
    "    shallow_mlp_model = keras.Sequential(\n",
    "        [\n",
    "            layers.Embedding(maxWords,embeddingDim, input_length=X_train.shape[1]),\n",
    "            layers.SpatialDropout1D(0.1),\n",
    "            layers.LSTM(100, dropout=0.1, recurrent_dropout=0.1),\n",
    "            layers.Dense(153, activation=\"softmax\"),\n",
    "        ]  # More on why \"sigmoid\" has been used here in a moment.\n",
    "    )\n",
    "    return shallow_mlp_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MXKd7yIMuPCT"
   },
   "source": [
    "## Train the model\n",
    "\n",
    "We will train our model using the binary crossentropy loss. This is because the labels\n",
    "are not disjoint. For a given abstract, we may have multiple categories. So, we will\n",
    "divide the prediction task into a series of multiple binary classification problems. This\n",
    "is also why we kept the activation function of the classification layer in our model to\n",
    "sigmoid. Researchers have used other combinations of loss function and activation\n",
    "function as well. For example, in\n",
    "[Exploring the Limits of Weakly Supervised Pretraining](https://arxiv.org/abs/1805.00932),\n",
    "Mahajan et al. used the softmax activation function and cross-entropy loss to train\n",
    "their models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KaDe2xwxuPCT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "464/464 [==============================] - 114s 241ms/step - loss: 0.0559 - accuracy: 0.5064 - val_loss: 0.0280 - val_accuracy: 0.5732\n",
      "Epoch 2/20\n",
      "464/464 [==============================] - 112s 242ms/step - loss: 0.0277 - accuracy: 0.5308 - val_loss: 0.0277 - val_accuracy: 0.4213\n",
      "Epoch 3/20\n",
      "464/464 [==============================] - 112s 241ms/step - loss: 0.0276 - accuracy: 0.5320 - val_loss: 0.0278 - val_accuracy: 0.4213\n",
      "Epoch 4/20\n",
      "464/464 [==============================] - 112s 241ms/step - loss: 0.0276 - accuracy: 0.5409 - val_loss: 0.0274 - val_accuracy: 0.5978\n",
      "Epoch 5/20\n",
      "464/464 [==============================] - 112s 241ms/step - loss: 0.0266 - accuracy: 0.6664 - val_loss: 0.0263 - val_accuracy: 0.6599\n",
      "Epoch 6/20\n",
      "363/464 [======================>.......] - ETA: 24s - loss: 0.0251 - accuracy: 0.7279"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "trainingDatset = tf.data.Dataset.from_tensor_slices(X_train)\n",
    "validationDataset = tf.data.Dataset.from_tensor_slices(X_val)\n",
    "\n",
    "shallow_mlp_model = make_model()\n",
    "shallow_mlp_model.compile(\n",
    "    loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "history = shallow_mlp_model.fit(\n",
    "    X_train, train_label_binarized, epochs=epochs, batch_size = 64, validation_split=0.1, callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)]\n",
    ")\n",
    "\n",
    "\n",
    "def plot_result(item):\n",
    "    plt.plot(history.history[item], label=item)\n",
    "    plt.plot(history.history[\"val_\" + item], label=\"val_\" + item)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(item)\n",
    "    plt.title(\"Train and Validation {} Over Epochs\".format(item), fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_result(\"loss\")\n",
    "plot_result(\"categorical_accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OTB3V-ysuPCT"
   },
   "source": [
    "While training, we notice an initial sharp fall in the loss followed by a gradual decay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dsUuAlfTuPCT"
   },
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VI4jrSFbuPCU"
   },
   "outputs": [],
   "source": [
    "_, categorical_acc = shallow_mlp_model.evaluate(test_dataset)\n",
    "print(f\"Categorical accuracy on the test set: {round(categorical_acc * 100, 2)}%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X7zHKRuFuPCU"
   },
   "source": [
    "The trained model gives us an evaluation accuracy of ~87%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j9eoo4R6uPCU"
   },
   "source": [
    "## Inference\n",
    "\n",
    "An important feature of the\n",
    "[preprocessing layers provided by Keras](https://keras.io/guides/preprocessing_layers/)\n",
    "is that they can be included inside a `tf.keras.Model`. We will export an inference model\n",
    "by including the `text_vectorization` layer on top of `shallow_mlp_model`. This will\n",
    "allow our inference model to directly operate on raw strings.\n",
    "\n",
    "**Note** that during training it is always preferable to use these preprocessing\n",
    "layers as a part of the data input pipeline rather than the model to avoid\n",
    "surfacing bottlenecks for the hardware accelerators. This also allows for\n",
    "asynchronous data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XOYdnkuRuPCV"
   },
   "outputs": [],
   "source": [
    "# Create a model for inference.\n",
    "model_for_inference = keras.Sequential([text_vectorizer, shallow_mlp_model])\n",
    "\n",
    "# Create a small dataset just for demoing inference.\n",
    "inference_dataset = make_dataset(test_df.sample(100), is_train=False)\n",
    "text_batch, label_batch = next(iter(inference_dataset))\n",
    "predicted_probabilities = model_for_inference.predict(text_batch)\n",
    "\n",
    "# Perform inference.\n",
    "for i, text in enumerate(text_batch[:5]):\n",
    "    label = label_batch[i].numpy()[None, ...]\n",
    "    print(f\"Abstract: {text[0]}\")\n",
    "    print(f\"Label(s): {invert_multi_hot(label[0])}\")\n",
    "    predicted_proba = [proba for proba in predicted_probabilities[i]]\n",
    "    top_3_labels = [\n",
    "        x\n",
    "        for _, x in sorted(\n",
    "            zip(predicted_probabilities[i], lookup.get_vocabulary()),\n",
    "            key=lambda pair: pair[0],\n",
    "            reverse=True,\n",
    "        )\n",
    "    ][:3]\n",
    "    print(f\"Predicted Label(s): ({', '.join([label for label in top_3_labels])})\")\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z067hmqquPCV"
   },
   "source": [
    "The prediction results are not that great but not below the par for a simple model like\n",
    "ours. We can improve this performance with models that consider word order like LSTM or\n",
    "even those that use Transformers ([Vaswani et al.](https://arxiv.org/abs/1706.03762))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8rjGoTMZuPCV"
   },
   "source": [
    "## Acknowledgements\n",
    "\n",
    "We would like to thank [Matt Watson](https://github.com/mattdangerw) for helping us\n",
    "tackle the multi-label binarization part and inverse-transforming the processed labels\n",
    "to the original form."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LSTM",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
